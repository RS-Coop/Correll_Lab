<%BEGIN_DEFINITION_TEMPLATE>
/********************
    activations.cpp

    Code generated using nn4mc.

    This file implements all activation functions.

*/

#include "activations.h"
#include <math.h>
#include <stdlib.h>

#define max(a, b) (((a)>(b) ? (a) : (b)))
#define min(a, b) (((a)<(b) ? (a) : (b)))

<%LAYER_DATATYPE_DELIMITER> sigmoid(<%LAYER_DATATYPE_DELIMITER> input)
{
  if (input >= 0.0){
	input = 1./ (1. + exp(-1.* input));
  } else{	
  	input = exp(input)/(exp(input) + 1.);
  }
  return input;
}

<%LAYER_DATATYPE_DELIMITER> softplus(<%LAYER_DATATYPE_DELIMITER> input)
{
  input = log(exp(input) + 1);

  return input;
}

<%LAYER_DATATYPE_DELIMITER> softsign(<%LAYER_DATATYPE_DELIMITER> input)
{
  input = input / (abs(input) + 1);

  return input;
}

<%LAYER_DATATYPE_DELIMITER> hard_sigmoid(<%LAYER_DATATYPE_DELIMITER> input)
{

  input = 0.2*input + 0.5;
  if (input <= 0.0){
	input = 0.0;
  }else{
	if (input >= 1.){
	    input = 1.0;
	}
  } 
  return input;
}

<%LAYER_DATATYPE_DELIMITER> exponential(<%LAYER_DATATYPE_DELIMITER> input)
{
  input = (<%LAYER_DATATYPE_DELIMITER>)expf((float)input);

  return input;
}

<%LAYER_DATATYPE_DELIMITER> elu(<%LAYER_DATATYPE_DELIMITER> input, <%LAYER_DATATYPE_DELIMITER> alpha)
{
  if (input > 0){
    input = 0.0;
  } else{
	input = alpha * (exp(input) - 1.);
  }
  return input;
}

<%LAYER_DATATYPE_DELIMITER> selu(<%LAYER_DATATYPE_DELIMITER> input)
{
  int scale = 1.0507009873554804934193349852946;
  int alpha = 1.6732632423543772848170429916717; 
  if (input > 0.0) {
      input *= scale; 
  } else{
 	input = exp(input) - 1.) * scale * alpha;
	}
  return input;
}

<%LAYER_DATATYPE_DELIMITER> relu(<%LAYER_DATATYPE_DELIMITER> input)
{
  input = max(input, 0.0);

  return input;
}

<%LAYER_DATATYPE_DELIMITER> tanh(<%LAYER_DATATYPE_DELIMITER> input)
{
  return tanh(input);
}

<%LAYER_DATATYPE_DELIMITER> softmax(<%LAYER_DATATYPE_DELIMITER> input, <%INDEX_DATATYPE_DELIMITER> output_shape)
{
  float sum_exp = 0.0;
  for (int i=0; i<output_shape; i++){
      sum_exp+= expf(input);
  }
  for (int i=0; i<output_shape;i++){
      float calc = expf(input) / sum_exp;
      if (isnan(calc)){
          input = 1.0;
      } else input = (<%LAYER_DATATYPE_DELIMITER>)(expf(input) / sum_exp);
  }

  return input;
}

//NOTE: This is deprecated and need to be changed to be more streamlined
<%LAYER_DATATYPE_DELIMITER> activate(<%LAYER_DATATYPE_DELIMITER> input, <%INDEX_DATATYPE_DELIMITER> output_shape, char type)
{
  if (type == 0x00)
    return softmax(input, output_shape);

   else if (type == 0x02)
    return elu(input, 0.5);
	
   else if (type == 0x03)
    return selu(input); 

  else if (type == 0x04)
    return softplus(input);

  else if (type == 0x05)
    return softsign(input);

  else if (type == 0x06)
    return relu(input);

  else if (type == 0x07)
    return tanh(input);

  else if (type == 0x08)
    return sigmoid(input);

  else if (type == 0x09)
    return hard_sigmoid(input);

  else if (type == 0xA)
    return exponential(input);

  return input;
}

<%END_DEFINITION_TEMPLATE>
